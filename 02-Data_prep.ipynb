{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import collections\n",
    "import arviz as az\n",
    "import pystan\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gplvm_predictions(Q, kernel, path, data):\n",
    "    for Q in Qs:\n",
    "        for kernel in kernels:\n",
    "            samples = pd.read_csv(path+'samples/s_Q{}_kernel{}.csv'.format(Q, kernel), sep=',', comment='#')\n",
    "\n",
    "            ## COVARIANCE MATRIX ##\n",
    "            K_ = samples.filter(regex=(\"^K\"))\n",
    "            K_ = K_.replace([np.inf, -np.inf], np.nan).dropna(0).mean(axis=0)\n",
    "            K_mat = K_.to_numpy().reshape(N,N)\n",
    "            K = pd.DataFrame(K_mat)\n",
    "            K.to_csv(path+'samples/K_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "\n",
    "            ## FUNCTION NOISE ##\n",
    "            noise_std_ = samples.filter(regex=(\"^noise_std\"))\n",
    "            noise_std_.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            noise_std_np = noise_std_.mean(axis=0)\n",
    "            noise_std = pd.DataFrame(noise_std_np)\n",
    "            noise_std.to_csv(path+'samples/noise_std_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "\n",
    "            ## KERNEL NOISE ##\n",
    "            kernel_std__ = samples.filter(regex=(\"^diag_stds\"))\n",
    "            kernel_std__.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            kernel_std_ = kernel_std__.mean(axis=0)\n",
    "            kernel_std_np = kernel_std_.to_numpy().reshape(N)\n",
    "            kernel_std = pd.DataFrame(kernel_std_np)\n",
    "            kernel_std = kernel_std.rename_axis('D')\n",
    "            kernel_std = kernel_std.rename_axis('N', axis='columns')\n",
    "            kernel_std.to_csv(path+'samples/kernel_std_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "            \n",
    "            ## LATENT SPACE POSITIONS ##\n",
    "            X__ = samples.filter(regex=(\"^X\"))\n",
    "            X__.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            X_ = X__.mean(axis=0)\n",
    "            X_np = X_.to_numpy().reshape(Q,N)\n",
    "            X = pd.DataFrame(X_np)\n",
    "            X = X.rename_axis('Latent Dimension')\n",
    "            X = X.rename_axis('Stock', axis='columns')\n",
    "            X.to_csv(path+'samples/X_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "\n",
    "            ## PREDICTIONS ##\n",
    "            K_noise = np.add(K, np.matmul(np.eye(N), noise_std))\n",
    "            prod1 = np.matmul(np.linalg.pinv(K_noise.to_numpy()),data)\n",
    "            predictions_ = np.matmul(K.values, prod1)\n",
    "            predictions = pd.DataFrame(predictions_)\n",
    "            predictions = predictions.rename_axis('D')\n",
    "            predictions = predictions.rename_axis('N', axis='columns')\n",
    "            predictions.to_csv(path+'samples/Y_hat-Q{}-kernel{}.csv'.format(Q, kernel))\n",
    "            model_analysis(Q, kernel, path, data, predictions, samples)\n",
    "    return K, noise_std, kernel_std, X, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def studt_predictions(Qs, kernels, paths, data):\n",
    "    for Q in Qs:\n",
    "        for kernel in kernels:\n",
    "            samples = pd.read_csv(path+'samples/s_Q{}_kernel{}.csv'.format(Q, kernel), sep=',', comment='#')\n",
    "\n",
    "            ## COVARIANCE MATRIX ##\n",
    "            K_ = samples.filter(regex=(\"^K\"))\n",
    "            K_ = K_.replace([np.inf, -np.inf], np.nan).dropna(0).mean(axis=0)\n",
    "            K_mat = K_.to_numpy().reshape(N,N)\n",
    "            K = pd.DataFrame(K_mat)\n",
    "            K.to_csv(path+'samples/K_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "\n",
    "            ## FUNCTION NOISE ##\n",
    "            noise_std_ = samples.filter(regex=(\"^noise_std\"))\n",
    "            noise_std_.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            noise_std_np = noise_std_.mean(axis=0)\n",
    "            noise_std = pd.DataFrame(noise_std_np)\n",
    "            noise_std.to_csv(path+'samples/noise_std_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "\n",
    "            ## KERNEL NOISE ##\n",
    "            kernel_std__ = samples.filter(regex=(\"^diag_stds\"))\n",
    "            kernel_std__.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            kernel_std_ = kernel_std__.mean(axis=0)\n",
    "            kernel_std_np = kernel_std_.to_numpy().reshape(N)\n",
    "            kernel_std = pd.DataFrame(kernel_std_np)\n",
    "            kernel_std = kernel_std.rename_axis('D')\n",
    "            kernel_std = kernel_std.rename_axis('N', axis='columns')\n",
    "            kernel_std.to_csv(path+'samples/kernel_std_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "            \n",
    "            ## LATENT SPACE POSITIONS ##\n",
    "            X__ = samples.filter(regex=(\"^X\"))\n",
    "            X__.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            X_ = X__.mean(axis=0)\n",
    "            X_np = X_.to_numpy().reshape(Q,N)\n",
    "            X = pd.DataFrame(X_np)\n",
    "            X = X.rename_axis('Latent Dimension')\n",
    "            X = X.rename_axis('Stock', axis='columns')\n",
    "            X.to_csv(path+'samples/X_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "\n",
    "            ## PREDICTIONS ##\n",
    "            K_noise = np.add(K, np.matmul(np.eye(N), noise_std))\n",
    "            prod1 = np.matmul(np.linalg.pinv(K_noise.to_numpy()),data)\n",
    "            predictions_ = np.matmul(K.values, prod1)\n",
    "            predictions = pd.DataFrame(predictions_)\n",
    "            predictions = predictions.rename_axis('D')\n",
    "            predictions = predictions.rename_axis('N', axis='columns')\n",
    "            predictions.to_csv(path+'samples/Y_hat-Q{}-kernel{}.csv'.format(Q, kernel))\n",
    "            model_analysis(Q, kernel, path, data, predictions, samples)\n",
    "            \n",
    "            ## DEGREE OF FREEDOM ##\n",
    "            t_nu__ = samples.filter(regex=(\"^t_nu\"))\n",
    "            t_nu__.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            t_nu_ = t_nu__.mean(axis=0)\n",
    "            t_nu = pd.DataFrame(t_nu_)\n",
    "            t_nu.to_csv(path+'samples/kernel_std_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "    return K, noise_std, kernel_std, kernel_std, t_nu, X, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vola_predictions(Qs, kernels, paths, data):\n",
    "    for Q in Qs:\n",
    "        for kernel in kernels:\n",
    "            samples = pd.read_csv(path+'samples/s_Q{}_kernel{}.csv'.format(Q, kernel), sep=',', comment='#')\n",
    "\n",
    "            ## COVARIANCE MATRIX ##\n",
    "            K__ = samples.filter(regex=(\"^K\"))\n",
    "            K_ = K__.replace([np.inf, -np.inf], np.nan).dropna(0).mean(axis=0)\n",
    "            K_mat = K_.to_numpy().reshape((D,N,N), order='F')\n",
    "            K = pd.DataFrame(K_mat)\n",
    "            #K_.to_csv(path+'samples/K_Q{}_kernel{}.csv'.format(Q, kernel))\n",
    "\n",
    "            ## INTRINSIC NOISE ##\n",
    "            noise_std_ = samples.filter(regex=(\"^noise_std\"))\n",
    "            noise_std_.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            noise_std_np = noise_std_.mean(axis=0)\n",
    "            noise_std = pd.DataFrame(noise_std_np)\n",
    "            noise_std.to_csv(path+'samples/noise_std_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "\n",
    "            ## LATENT SPACE POSITIONS ##\n",
    "            X__ = samples.filter(regex=(\"^X\"))\n",
    "            X__.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            X_ = X__.mean(axis=0)\n",
    "            X_np = X_.to_numpy().reshape(Q,N)\n",
    "            X = pd.DataFrame(X_np)\n",
    "            X = X.rename_axis('Latent Dimension')\n",
    "            X = X.rename_axis('Stock', axis='columns')\n",
    "            X.to_csv(path+'samples/X_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "\n",
    "            ## VOLATILITIES ##\n",
    "            Sigma__ = samples.filter(regex=(\"^Sigma\"))\n",
    "            Sigma__.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            Sigma_ = Sigma__.mean(axis=0)\n",
    "            Sigma_np = Sigma_.to_numpy().reshape(D,N)\n",
    "            Sigma = pd.DataFrame(Sigma_np)\n",
    "            Sigma = Sigma.rename_axis('D')\n",
    "            Sigma = Sigma.rename_axis('N', axis='columns')\n",
    "            Sigma.to_csv(path+'samples/Sigma_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "\n",
    "            ## PREDICTIONS ##\n",
    "            predictions = pd.DataFrame()\n",
    "            for d in range(-1,D-1):\n",
    "                K_noise = np.add(pd.DataFrame(K_mat[d,:,:]), np.matmul(np.eye(N), noise_std_np))\n",
    "                prod1 = np.matmul(np.linalg.pinv(K_noise),data)\n",
    "                predictions___ = pd.DataFrame(np.matmul(K_mat[d,:,:], pd.DataFrame(prod1)))\n",
    "                predictions__ = pd.DataFrame(predictions___.iloc[:,d])\n",
    "                predictions = pd.concat([predictions,predictions__], axis=1, ignore_index=True)\n",
    "            predictions.to_csv(path+'samples/Y_hat-Q{}-kernel{}.csv'.format(Q, kernel))\n",
    "            model_analysis(Q, kernel, path, data, predictions, samples)\n",
    "    return K, noise_std, X, Sigma, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_predictions(Qs, kernels, paths, data):\n",
    "    for Q in Qs:\n",
    "        for kernel in kernels:\n",
    "            samples = pd.read_csv(path+'samples/s_Q{}_kernel{}.csv'.format(Q, kernel), sep=',', comment='#')\n",
    "\n",
    "            ## COVARIANCE MATRIX ##\n",
    "            K_y__ = samples.filter(regex=(\"^K_y\"))\n",
    "            K_y_ = K_y__.replace([np.inf, -np.inf], np.nan).dropna(0).mean(axis=0)\n",
    "            K_y_mat = K_y_.to_numpy().reshape((D,N,N), order='F')\n",
    "            K_y_.to_csv(path+'samples/K_y_Q{}_kernel{}.csv'.format(Q, kernel))\n",
    "\n",
    "            ## INTRINSIC NOISE ##\n",
    "            noise_std_x_ = samples.filter(regex=(\"^noise_std_x\"))\n",
    "            noise_std_x_.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            noise_std_x_np = noise_std_x_.mean(axis=0)\n",
    "            noise_std_x = pd.DataFrame(noise_std_x_np)\n",
    "            noise_std_x.to_csv(path+'noise_std_x_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "\n",
    "            noise_std_y_ = samples.filter(regex=(\"^noise_std_y\"))\n",
    "            noise_std_y_.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            noise_std_y_np = noise_std_y_.mean(axis=0)\n",
    "            noise_std_y = pd.DataFrame(noise_std_y_np)\n",
    "            noise_std_y.to_csv(path+'noise_std_y_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "\n",
    "            ## KERNEL NOISE ##\n",
    "            kernel_std_x__ = samples.filter(regex=(\"^kernel_std_x\"))\n",
    "            kernel_std_x__.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            kernel_std_x_ = kernel_std_x__.mean(axis=0)\n",
    "            kernel_std_x_np = kernel_std_x_.to_numpy()\n",
    "            kernel_std_x = pd.DataFrame(kernel_std_x_np)\n",
    "            kernel_std_x = kernel_std_x.rename_axis('D')\n",
    "            kernel_std_x = kernel_std_x.rename_axis('N', axis='columns')\n",
    "            kernel_std_x.to_csv(path+'kernel_std_x_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "\n",
    "            kernel_std_y__ = samples.filter(regex=(\"^kernel_std_y\"))\n",
    "            kernel_std_y__.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            kernel_std_y_ = kernel_std_y__.mean(axis=0)\n",
    "            kernel_std_y_np = kernel_std_y_.to_numpy()\n",
    "            kernel_std_y = pd.DataFrame(kernel_std_x_np)\n",
    "            kernel_std_y = kernel_std_y.rename_axis('D')\n",
    "            kernel_std_y = kernel_std_y.rename_axis('N', axis='columns')\n",
    "            kernel_std_y.to_csv(path+'kernel_std_y_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "\n",
    "            ## LATENT SPACE POSITIONS ##\n",
    "            X__ = samples.filter(regex=(\"^X\"))\n",
    "            X__.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            X_ = X__.mean(axis=0)\n",
    "            prod = N*D\n",
    "            X_temp = pd.DataFrame()\n",
    "            for q in range(0,Q):\n",
    "                X_np_temp = X_.iloc[(q*prod):((q+1)*prod)].to_numpy().reshape(N,D)\n",
    "                X_temp_ = pd.DataFrame(X_np_temp)\n",
    "                X_temp_ = X_temp_.rename_axis(f'Q{q+1}')\n",
    "                X_temp = pd.concat([X_temp, X_temp_])\n",
    "            X = X_temp.rename_axis('Stock No.')\n",
    "            X = X.rename_axis('Days', axis='columns')\n",
    "            X.to_csv(path+'X_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "\n",
    "            ## PREDICTIONS ##\n",
    "            predictions = pd.DataFrame()\n",
    "            for d in range(-1,D-1):\n",
    "                K_noise = np.add(pd.DataFrame(K_y_mat[d,:,:]), np.matmul(np.eye(N), noise_std_y_np))\n",
    "                prod1 = np.matmul(np.linalg.pinv(K_noise),data)\n",
    "                predictions___ = pd.DataFrame(np.matmul(K_y_mat[d,:,:], pd.DataFrame(prod1)))\n",
    "                predictions__ = pd.DataFrame(predictions___.iloc[:,d])\n",
    "                predictions = pd.concat([predictions,predictions__], axis=1, ignore_index=True)\n",
    "            predictions.to_csv(path+'samples/Y_hat-Q{}-kernel{}.csv'.format(Q, kernel))\n",
    "            model_analysis(Q, kernel, path, data, predictions, samples)\n",
    "    return K_y, noise_std_x, noise_std_y, X, kernel_std_x, kernel_std_y, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timet_predictions(Qs, kernels, paths, data):\n",
    "    for Q in Qs:\n",
    "        for kernel in kernels:\n",
    "            samples = pd.read_csv(path+'samples/s_Q{}_kernel{}.csv'.format(Q, kernel), sep=',', comment='#')\n",
    "\n",
    "            ## COVARIANCE MATRIX ##\n",
    "            K_y__ = samples.filter(regex=(\"^K_y\"))\n",
    "            K_y_ = K_y__.replace([np.inf, -np.inf], np.nan).dropna(0).mean(axis=0)\n",
    "            K_y_mat = K_y_.to_numpy().reshape((D,N,N), order='F')\n",
    "            K_y_.to_csv(path+'samples/K_y_Q{}_kernel{}.csv'.format(Q, kernel))\n",
    "\n",
    "            ## INTRINSIC NOISE ##\n",
    "            noise_std_x_ = samples.filter(regex=(\"^noise_std_x\"))\n",
    "            noise_std_x_.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            noise_std_x_np = noise_std_x_.mean(axis=0)\n",
    "            noise_std_x = pd.DataFrame(noise_std_x_np)\n",
    "            noise_std_x.to_csv(path+'noise_std_x_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "\n",
    "            noise_std_y_ = samples.filter(regex=(\"^noise_std_y\"))\n",
    "            noise_std_y_.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            noise_std_y_np = noise_std_y_.mean(axis=0)\n",
    "            noise_std_y = pd.DataFrame(noise_std_y_np)\n",
    "            noise_std_y.to_csv(path+'noise_std_y_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "\n",
    "            ## KERNEL NOISE ##\n",
    "            kernel_std_x__ = samples.filter(regex=(\"^kernel_std_x\"))\n",
    "            kernel_std_x__.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            kernel_std_x_ = kernel_std_x__.mean(axis=0)\n",
    "            kernel_std_x_np = kernel_std_x_.to_numpy()\n",
    "            kernel_std_x = pd.DataFrame(kernel_std_x_np)\n",
    "            kernel_std_x = kernel_std_x.rename_axis('D')\n",
    "            kernel_std_x = kernel_std_x.rename_axis('N', axis='columns')\n",
    "            kernel_std_x.to_csv(path+'kernel_std_x_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "\n",
    "            kernel_std_y__ = samples.filter(regex=(\"^kernel_std_y\"))\n",
    "            kernel_std_y__.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            kernel_std_y_ = kernel_std_y__.mean(axis=0)\n",
    "            kernel_std_y_np = kernel_std_y_.to_numpy()\n",
    "            kernel_std_y = pd.DataFrame(kernel_std_x_np)\n",
    "            kernel_std_y = kernel_std_y.rename_axis('D')\n",
    "            kernel_std_y = kernel_std_y.rename_axis('N', axis='columns')\n",
    "            kernel_std_y.to_csv(path+'kernel_std_y_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "            \n",
    "            ## DEGREE OF FREEDOM ##\n",
    "            t_nu__ = samples.filter(regex=(\"^t_nu\"))\n",
    "            t_nu__.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            t_nu_ = t_nu__.mean(axis=0)\n",
    "            t_nu = pd.DataFrame(t_nu_)\n",
    "            t_nu.to_csv(path+'samples/kernel_std_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "\n",
    "            ## LATENT SPACE POSITIONS ##\n",
    "            X__ = samples.filter(regex=(\"^X\"))\n",
    "            X__.replace([np.inf, -np.inf], np.nan).dropna(0)\n",
    "            X_ = X__.mean(axis=0)\n",
    "            prod = N*D\n",
    "            X_temp = pd.DataFrame()\n",
    "            for q in range(0,Q):\n",
    "                X_np_temp = X_.iloc[(q*prod):((q+1)*prod)].to_numpy().reshape(N,D)\n",
    "                X_temp_ = pd.DataFrame(X_np_temp)\n",
    "                X_temp_ = X_temp_.rename_axis(f'Q{q+1}')\n",
    "                X_temp = pd.concat([X_temp, X_temp_])\n",
    "            X = X_temp.rename_axis('Stock No.')\n",
    "            X = X.rename_axis('Days', axis='columns')\n",
    "            X.to_csv(path+'X_Q{}_kernel{}.csv'.format(Q, kernel), index=False)\n",
    "\n",
    "            ## PREDICTIONS ##\n",
    "            predictions = pd.DataFrame()\n",
    "            for d in range(-1,D-1):\n",
    "                K_noise = np.add(pd.DataFrame(K_y_mat[d,:,:]), np.matmul(np.eye(N), noise_std_y_np))\n",
    "                prod1 = np.matmul(np.linalg.pinv(K_noise),data)\n",
    "                predictions___ = pd.DataFrame(np.matmul(K_y_mat[d,:,:], pd.DataFrame(prod1)))\n",
    "                predictions__ = pd.DataFrame(predictions___.iloc[:,d])\n",
    "                predictions = pd.concat([predictions,predictions__], axis=1, ignore_index=True)\n",
    "            predictions.to_csv(path+'samples/Y_hat-Q{}-kernel{}.csv'.format(Q, kernel))\n",
    "            model_analysis(Q, kernel, path, data, predictions, samples)\n",
    "    return K, noise_std, X, Sigma, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(diagnostic, path, Q, kernel, data, predictions):\n",
    "    elbo = diagnostic.loc[:, 'ELBO'].values[-1]\n",
    "    R2_skl = skl.metrics.r2_score(data, predictions)\n",
    "    R2_az = az.r2_score(data, predictions)\n",
    "    ind = ['kernel', 'Q', 'ELBO', 'R2_skl', 'R2_az']\n",
    "    num = np.random.random_integers(10000)\n",
    "    mod_res = pd.DataFrame({num: (kernel, Q, elbo, R2_skl, R2_az)}, index=ind).T\n",
    "    mod_res.to_csv(path+'eval_Q{}_kernel{}.csv'.format(Q, kernel))\n",
    "    return mod_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_plotter(variable_mean, variable_name, path, Q, kernel):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    variable_np = np.asarray(variable_mean)\n",
    "    np.delete(variable_np, 0, 1)\n",
    "    np.delete(variable_np, 0, 1)\n",
    "\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax = sns.distplot(variable_mean)\n",
    "    ax.set_xlabel('values')\n",
    "    ax.set_ylabel('value counts')\n",
    "    ax.set_title(variable_name + \" histogram\")\n",
    "\n",
    "    plt.savefig(path+'{}-hist_Q{}_kernel{}.png'.format(variable_name, Q, kernel))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_comp_plot(predictions, data, Q, path, kernel):\n",
    "    fig = plt.figure(figsize=(40,20))\n",
    "    x1, y1 = np.linspace(-0.1, 0.4, num=1000), np.linspace(-0.1, 0.4, num=1000)\n",
    "    for n in range(11):\n",
    "        sns.jointplot(x=predictions.iloc[n,:], y=data.iloc[n,:], data=predictions, kind=\"reg\", truncate=False)\n",
    "        plt.title(f'Returns of Stock {n}, Data vs Prediction')\n",
    "        plt.legend(['Prediction vs Observation', 'Ideal'])\n",
    "        plt.xlim(-0.1, 0.4)\n",
    "        plt.ylim(-0.1, 0.4)\n",
    "        plt.plot(x1, y1)\n",
    "        plt.xlabel('Predictions')\n",
    "        plt.ylabel('Observations/Data')\n",
    "        plt.savefig(path+'Q{}_kernel{}_stock{}.png'.format(Q, kernel, n))\n",
    "        plt.close()\n",
    "        \n",
    "        sns.jointplot(x=predictions.iloc[n,:], y=data.iloc[n,:], data=predictions, kind=\"kde\", truncate=False)\n",
    "        plt.title('Returns of Stock {}, Data vs Prediction'.format(n))\n",
    "        plt.legend(['Prediction vs Observation', 'Ideal'])\n",
    "        plt.xlim(-0.1, 0.4)\n",
    "        plt.ylim(-0.1, 0.4)\n",
    "        plt.plot(x1, y1)\n",
    "        plt.xlabel('Predictions')\n",
    "        plt.ylabel('Observations/Data')\n",
    "        plt.savefig(path+'Q{}_kernel{}_stock{}.png'.format(Q, kernel, n))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StanModel_cache(model_code, model_name):\n",
    "    cache_fn = f'cached-model-{model_name}.pkl'\n",
    "    try:\n",
    "        sm = pickle.load(open(cache_fn, 'rb'))\n",
    "    except:\n",
    "        sm = pystan.StanModel(model_code=model_code)\n",
    "        with open(cache_fn, 'wb') as f:\n",
    "            pickle.dump(sm, f)\n",
    "    else:\n",
    "        print(\"Using cached StanModel\")\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folders(model_name, time_period_name, ticker_list_name):\n",
    "    if not os.path.exists(f'{model_name}/{time_period_name}-{ticker_list_name}/diagnostics/'):\n",
    "        os.makedirs(f'{model_name}/{time_period_name}-{ticker_list_name}/diagnostics/')\n",
    "    if not os.path.exists(f'{model_name}/{time_period_name}-{ticker_list_name}/samples/'):\n",
    "        os.makedirs(f'{model_name}/{time_period_name}-{ticker_list_name}/samples/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_analysis(Q, kernel, path, data, predictions, samples):\n",
    "    model = LinearRegression()\n",
    "    comp_ = []\n",
    "    data = pd.DataFrame(data)\n",
    "    diagnostic = pd.read_csv(path+'diagnostics/d_Q{}_kernel{}.csv'.format(Q, kernel), header=20)\n",
    "    for n in range(1,N):\n",
    "        a = predictions.iloc[n,:].values.reshape(-1,1)\n",
    "        b = data.iloc[n-1,:]\n",
    "        try:\n",
    "            fit, R_2, intercept = model.fit(a,b), model.score(a,b), model.intercept_\n",
    "        except ValueError:\n",
    "            a, b = pd.DataFrame(a), pd.DataFrame(b)\n",
    "            fit, R_2, intercept = model.fit(a,b), model.score(a,b), model.intercept_\n",
    "        slope, = model.coef_\n",
    "        comp__ = collections.OrderedDict()\n",
    "        comp__ = {'R_2_fit':R_2, 'slope_fit':slope, 'intercept_fit':intercept}    \n",
    "        comp_.append(comp__)\n",
    "    comp = pd.DataFrame(comp_)\n",
    "    comp.index.name='N'\n",
    "    comp.to_csv(path+'diagnostics/comparison_Q{}_kernel{}.csv'.format(Q, kernel))\n",
    "    model_eval(diagnostic, path, Q, kernel, data, predictions)\n",
    "    hist_plotter(predictions, 'Y_hat', path, Q, kernel)\n",
    "    quick_comp_plot(predictions, data, Q, path, kernel)\n",
    "\n",
    "    #bfmi.to_csv('diagnostics/bfmi_Q{}_kernel{}.csv'.format(Q, kernel))\n",
    "    #psis.to_csv('diagnostics/psis_Q{}_kernel{}.csv'.format(Q, kernel))\n",
    "    return comp, print('Done with Q {} kernel {}.'.format(Q, kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Qs = [1,2,3,4,5,6]\n",
    "#kernels = [1,2,3,4,5]\n",
    "#returns = pd.read_csv('https://raw.githubusercontent.com/Kloppel/Thesis/master/resources/n120-d754-returns.csv', index_col='Date', parse_dates=['Date']).iloc[2:, :]\n",
    "#Y = returns.T.values\n",
    "#N, D = Y.shape\n",
    "#data = Y\n",
    "\n",
    "## GPLVM ##\n",
    "#path = 'gplvm_finance/2010-trunc/'\n",
    "#gplvm_predictions(Qs, kernels, path, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Qs = [5,6]#1,2,3,4,5,6]\n",
    "#kernels = [1,2,3,4,5]\n",
    "#returns = pd.read_csv('https://raw.githubusercontent.com/Kloppel/Thesis/master/resources/n120-d754-returns.csv', index_col='Date', parse_dates=['Date']).iloc[2:, :]\n",
    "#Y = returns.T.values\n",
    "#N, D = Y.shape\n",
    "#data = Y\n",
    "\n",
    "## STUDT ##\n",
    "#path = 'student-t/2010-trunc/'\n",
    "#studt_predictions(Qs, kernels, path, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 50100 into shape (499,10,10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d3cdb7ee5de5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m## VOLA, size largest ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gplvm_vola/2010-N10-D501/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mvola_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-a359164097cb>\u001b[0m in \u001b[0;36mvola_predictions\u001b[0;34m(Qs, kernels, paths, data)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mK__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"^K\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mK_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mK_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m#K_.to_csv(path+'samples/K_Q{}_kernel{}.csv'.format(Q, kernel))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 50100 into shape (499,10,10)"
     ]
    }
   ],
   "source": [
    "Qs = [1,2,3,4,5,6]\n",
    "kernels = [1,2,3,4,5]\n",
    "returns = pd.read_csv('https://raw.githubusercontent.com/Kloppel/Thesis/master/resources/n10-d501-returns.csv', index_col='Date', parse_dates=['Date']).iloc[2:, :]\n",
    "Y = returns.T.values\n",
    "N, D = Y.shape\n",
    "data = Y\n",
    "\n",
    "## VOLA, size largest ##\n",
    "path = 'gplvm_vola/2010-N10-D501/'\n",
    "vola_predictions(Qs, kernels, path, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8279ec70cea6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mQs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkernels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreturns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://raw.githubusercontent.com/Kloppel/Thesis/master/resources/n10-d503-returns.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;31m# See https://github.com/python/mypy/issues/1297\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     )\n\u001b[1;32m    437\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;31m# TODO: fsspec can also handle HTTP via requests, but leaving this unchanged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 642\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "Qs = [1,2,3,4,5,6]\n",
    "kernels = [1,2,3,4,5]\n",
    "returns = pd.read_csv('https://raw.githubusercontent.com/Kloppel/Thesis/master/resources/n30-d122-returns.csv', index_col='Date', parse_dates=['Date']).iloc[2:, :]\n",
    "Y = returns.T.values\n",
    "N, D = Y.shape\n",
    "data = Y\n",
    "\n",
    "## VOLA, size like TIME ##\n",
    "path = 'gplvm_vola/2010-vola/'\n",
    "vola_predictions(Qs, kernels, path, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
